{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection Name: wildfire_collection_feature_engineering_final\n",
      "An error occurred: Cursor 4682574967188996 not found, full error: {'ok': 0.0, 'errmsg': 'Cursor 4682574967188996 not found', 'code': 43, 'codeName': 'CursorNotFound'}\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 34\u001b[0m\n\u001b[0;32m     29\u001b[0m df_historical\u001b[38;5;241m=\u001b[39mdf_hist\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# df_historical.shape\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# df_historical.head(5)\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# df_historical.info()\u001b[39;00m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# Normalize the data to ensure consistency\u001b[39;00m\n\u001b[1;32m---> 34\u001b[0m df_historical[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrep_date\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(\u001b[43mdf_historical\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrep_date\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m)\u001b[38;5;241m.\u001b[39mdt\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm-\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# Convert 'rep_date' back to datetime to filter by month and year\u001b[39;00m\n\u001b[0;32m     37\u001b[0m df_historical[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrep_date\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(df_historical[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrep_date\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "# pip install supabase pandas requests\n",
    "\n",
    "# Import Libraries\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd\n",
    "from pymongo import MongoClient\n",
    "from dotenv import load_dotenv\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "import math\n",
    "# Add the 'scripts' directory to the Python path\n",
    "sys.path.append(os.path.abspath(os.path.join('..', 'scripts')))\n",
    "import extract_to_mongodb as etm\n",
    "import db_utils as dbu\n",
    "import pickle\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "collection_name = os.getenv('COLLECTION_NAME_FEATUREENGINEERED_FINAL')\n",
    "naturalearth_lowres = os.getenv('NATURALEARTH_SHAPEFILE_PATH')\n",
    "# CACHE_FILE = 'geocodecache.pkl'\n",
    "print(f\"Collection Name: {collection_name}\")\n",
    "# Load the Data\n",
    "# Load the cleaned data\n",
    "df_hist = dbu.load_all_data_from_mongodb(collection_name)\n",
    "df_historical=df_hist\n",
    "# df_historical.shape\n",
    "# df_historical.head(5)\n",
    "# df_historical.info()\n",
    "# Normalize the data to ensure consistency\n",
    "df_historical['rep_date'] = pd.to_datetime(df_historical['rep_date']).dt.strftime('%Y-%m-%d')\n",
    "\n",
    "# Convert 'rep_date' back to datetime to filter by month and year\n",
    "df_historical['rep_date'] = pd.to_datetime(df_historical['rep_date'])\n",
    "\n",
    "# Filter for records from July to December 2023\n",
    "df_historical = df_historical[(df_historical['rep_date'].dt.year == 2023) & \n",
    "                              (df_historical['rep_date'].dt.month >= 7) & \n",
    "                              (df_historical['rep_date'].dt.month <= 12)]\n",
    "\n",
    "\n",
    "df_historical.head(5)\n",
    "# df_historical = df_historical.reset_index()\n",
    "# df_historical = (df_historical.merge((df_historical[['rep_date']].drop_duplicates(ignore_index=True).rename_axis('time_idx'))\\\n",
    "#                      .reset_index(), on = ['rep_date'])).drop(\"rep_date\", axis=1)\n",
    "df_historical.head(5)\n",
    "cols_to_keep = ['locality','elev', 'month', 'day', 'year', 'cfb', 'temp', 'wd', 'rh', 'pcuring', 'ros', 'hfi', 'tfc0', 'sfl', 'bui', 'cfl', 'sfc0', 'dmc', 'sfc', 'bfc', 'tfc', 'isi']\n",
    "df_historical = df_historical[cols_to_keep]\n",
    "# df_historical['cfb'] = df_historical['cfb'].astype(float)\n",
    "df_historical['fire_occurrence'] = (df_historical['cfb'] > 0).astype(int)\n",
    "def create_lagged_features(df, features, lags, fill_method='ffill_bfill'):\n",
    "    for feature in features:\n",
    "        for lag in lags:\n",
    "            df[f'{feature}_lag{lag}'] = df[feature].shift(lag)\n",
    "            if fill_method == 'ffill_bfill':\n",
    "                df[f'{feature}_lag{lag}'] = df[f'{feature}_lag{lag}'].fillna(method='ffill').fillna(method='bfill')\n",
    "            elif fill_method == 'ffill':\n",
    "                df[f'{feature}_lag{lag}'] = df[f'{feature}_lag{lag}'].fillna(method='ffill')\n",
    "            elif fill_method == 'bfill':\n",
    "                df[f'{feature}_lag{lag}'] = df[f'{feature}_lag{lag}'].fillna(method='bfill')\n",
    "    return df\n",
    "\n",
    "\n",
    "# Define lag features\n",
    "features_to_lag = ['cfb', 'dmc', 'temp', 'tfc', 'ros', 'pcuring', 'bfc', 'hfi']\n",
    "lags = [1, 2, 3, 5, 6, 7]\n",
    "\n",
    "# Create lagged features\n",
    "df_historical = create_lagged_features(df_historical, features_to_lag, lags)\n",
    "\n",
    "# Drop rows with NaN values created by lagging\n",
    "# df_historical.dropna(inplace=True)\n",
    "num_rows_with_nan = df_historical[cols].isnull().sum(axis=1).sum()\n",
    "print(f\"Number of rows with NaN values: {num_rows_with_nan}\")\n",
    "df_historical.head(5)\n",
    "duplicates = df.duplicated()\n",
    "print(\"Duplicate rows:\")\n",
    "print(df[duplicates])\n",
    "df_historical.isnull().sum()\n",
    "\n",
    "# df_historical.to_csv('historical_07_12.csv', index=False)\n",
    "# Save the cleaned data to mongodb\n",
    "# dbu.insert_df_only_to_mongodb(df_historical, 'wildfire_collection_2023_07_12')\n",
    "import pandas as pd\n",
    "import requests\n",
    "from supabase import create_client, Client\n",
    "\n",
    "# Create a Supabase client\n",
    "supabase_url = 'https://owflvfhmlpletxyjyhwv.supabase.co'\n",
    "supabase_key = 'eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6Im93Zmx2ZmhtbHBsZXR4eWp5aHd2Iiwicm9sZSI6ImFub24iLCJpYXQiOjE3MjI4ODQ0MzksImV4cCI6MjAzODQ2MDQzOX0.cuJoRQNwckv6fAIHKZnBjTW2akcqo9iMA5qz8ocS62A'\n",
    "supabase_client = Client(supabase_url, supabase_key)\n",
    "\n",
    "# Insert the JSON data into the Supabase table\n",
    "# Create the table in Supabase\n",
    "table_name = 'historical_07_12'\n",
    "# Function to map pandas dtypes to SQL types\n",
    "def map_dtype_to_sql(dtype):\n",
    "    if pd.api.types.is_integer_dtype(dtype):\n",
    "        return \"INTEGER\"\n",
    "    elif pd.api.types.is_float_dtype(dtype):\n",
    "        return \"FLOAT\"\n",
    "    elif pd.api.types.is_bool_dtype(dtype):\n",
    "        return \"BOOLEAN\"\n",
    "    elif pd.api.types.is_datetime64_any_dtype(dtype):\n",
    "        return \"TIMESTAMP\"\n",
    "    else:\n",
    "        return \"TEXT\"\n",
    "\n",
    "\n",
    "# Generate SQL CREATE TABLE query\n",
    "columns_sql = \",\\n    \".join([f\"{col} {map_dtype_to_sql(dtype)}\" for col, dtype in zip(df_historical.columns, df_historical.dtypes)])\n",
    "create_table_query = f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS {table_name} (\n",
    "    {columns_sql}\n",
    ");\n",
    "\"\"\"\n",
    "print(create_table_query)\n",
    "# Use the Supabase REST API to execute the SQL query\n",
    "# headers = {\n",
    "#     \"apikey\": supabase_key,\n",
    "#     \"Authorization\": f\"Bearer {supabase_key}\",\n",
    "#     \"Content-Type\": \"application/json\"\n",
    "# }\n",
    "\n",
    "# # API endpoint for executing SQL in Supabase\n",
    "# sql_endpoint = f\"{supabase_url}/rest/v1/rpc/execute_sql\"\n",
    "\n",
    "# # Execute the SQL query\n",
    "# response = requests.post(sql_endpoint, json={\"sql\": create_table_query}, headers=headers)\n",
    "\n",
    "# # Convert DataFrame to dictionary records\n",
    "# data_to_insert = df_historical.to_dict(orient=\"records\")\n",
    "# # Convert 'bui' column to integer\n",
    "# df_historical['bui'] = df_historical['bui'].astype(int)\n",
    "\n",
    "# # Convert DataFrame to dictionary records\n",
    "# data_to_insert = df_historical.to_dict(orient=\"records\")\n",
    "\n",
    "# # Insert the data into the table\n",
    "# insert_response = supabase_client.table(table_name).insert(data_to_insert).execute()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
